{"title":"K-mean and GMM Tutorial","markdown":{"yaml":{"title":"K-mean and GMM Tutorial","subtitle":"Gem City Tech ML/AI Meetup","date":"Nov. 17 2022","author":"Presented by Evelyn J. Boettcher, DiDacTex, LLC","format":{"revealjs":{"incremental":false}},"fig-cap-location":"margin"},"headingText":"Welcome to Gem City Tech ML/AI","containsRefs":false,"markdown":"\n\n\n\n:::{.columns}\n::::{.column width=72%}\n\nWe meet every third Thursday and are part of Gem City Tech meetup group\n\n<br>\n\nGemCity TECH’s mission is to grow the local industry and the community by providing a centralized destination for\ntechnical training, workshops and providing a forum for collaborating.\n\n<br>\n\nCurrently, support eight special interest groups from a variety of technical disciplines.\n::::\n::::{.column width=26%}\n![](./img/qr-code.png)\n::::\n:::\n\n- Dayton Web Developers\n- Dayton Dynamic Languages\n- Dayton .net Developers\n- Gem City Games Developments\n- New to Tech\n- Frameworks\n- **Machine Learning / Artificial Intelligence (ML/AI)**\n- Code for Dayton\n\n---\n\n## Big Thanks to our Sponsors\n\n<br>\n<br>\n\n:::{.columns}\n::::{.column width=28%}\n\n#### Gem City Tech\n![](./img/GCTLogo.PNG)\n\n::::\n::::{.column width=28%}\n\n#### The Hub\n ![](./img/The-Hub-Logo-PNC.jpg) \n::::\n::::{.column width=28%}\n#### Technology First \n ![](./img/tech_first_logo.jpg) \n\n::::\n:::\n\n\n* GemCity TECH: [GemCity.Tech](https://Gemcity.tech)\n* The Innovation Hub: [thehubdayton.com](https://www.thehubdayton.com/)\n* Technology First: [www.technologyfirst.org](https://www.technologyfirst.org/)\n\n---\n\n### Calendar Events\n\n* Gem City Tech ML/AI: Third Thursday at 6.\n* The GemCity TECH Meetup calendar of upcoming events: [www.meetup.com/gem-city-tech](https://www.meetup.com/gem-city-tech/events/calendar/)\n* [The Technology First events calendar](https://www.technologyfirst.org/Technology-First-Events?EventViewMode=1&EventListViewMode=2&SelectedDate=8/20/2022&CalendarViewType=1)\n\n---\n\n## Unsupervised Learning \n\nBefore we get into k-means.  Let's go over unsupervised learning.\nUnsupervised learning is when you have data but do not have the truth but need to classify.\n\n<br>\n\nSupervised learning is when you have labels for each data point.\n\n<br>\n<br>\n\n### Example of Data for Supervised Learning is \n\n| Data                     | Truth / Label | Data                                 |  Truth  /Label  |\n|:-------------------------|:--------------|:-------------------------------------|:--------------|\n| ![](./img/cat_test2.jpg){width=\"200px\"} | cat           | ![](./img/smores.jpg){width=\"200px\"} | dog           |\n\n---\n\n### Example of data for unsupervised learning\n\nSame data as above but without the Truth / Label\n\n\n\n\n| Data                     | Data                                   |\n|:-------------------------|:--------------|\n| ![](./img/cat_test2.jpg){width=\"200px\"} | ![](./img/smores.jpg){width=\"200px\"} | \n\n<br>\n\n### Goal of unsupervised learning\n\nFind features that separate the data into groups / clusters.\nThen hope there is a small number of labeled features so that we can classify those groups.\n\n### Why Do Unsupervised\n\nLabeled data is really expensive.  \n\n----\n\n## Gedunken Experiment\n\nSay there is a town with two employers (A and B).  \nPredict if person works for A or B, based on where they live.\n\n<br>\n\nFirst stab: See if people are clustered around their work place.\n\nSo if we had a clustering algorithm we can predict where a person might work.\n\n:::{ }\n\n![](./img/townpeople.png){width=55% fig-align=center}\n\n:::\n\n\n---\n\n### K-Means\n\nK-means is the most common clustering algorithm.\n<br>\nK-means clusters, n data points (e.g. All your data) into k clusters by placing each data point to the nearest k.\n\n<br>\n\nExample: point **p** is 5 ft from $k_i$ and 10 feet from $k_j$.  K-means would then place point **p** into the\n$k_i$ group.  Once, all the points are placed into a cluster or group, the new means for each cluster ($k_i$ and $k_j$) \nare updated.  The location for each mean cluster __k__ is calculated by finding the mean from all the points that belong to that cluster.\n\n<br>\n\n\n* Seed with initial k mean points\n* Find where each point belongs: cluster A or B\n* Calculate a new mean for A and B\n* Repeat steps 2 and 3.\n\n### Kmeans\n\nKmeans is just that, k means.\n\n---\n\n### Background \n\nk-means clustering comes from signal processing.  Invented in 1967 by MacQueen.<br>\nk-means clustering minimizes within-cluster\nvariances (squared Euclidean distances).  There is also k-medians and k-medoids.\n\n<br>\n\nK-means is an iterative method. (We repeated steps 2 and 3, till some threshold)\n\n<br>\n\nIt converges but it can converge to a local minium!<br>\nk-means and Gaussian mixture model both use cluster centers to model the data.\n\n<br>\n\nk-means clustering assumes each mean has same distribution, while the Gaussian mixture model \nallows clusters to have different distributions.\n\n---\n\n### K-means math\n\nRandomly create an initial set of k means: $\\mu_1$, ..., $\\mu_k$.\n\nAlternating between assignment and update steps:\n\n<br>\n\n#### Assignment step: \n\nAssign each data point to the cluster\nwith the nearest mean (least squared Euclidean distance).\n\nEach point is assigned to exactly **one** k mean.  Creating k sets ($S$)\n\n$$\nS_i = { x_p: | x_p - \\mu_i |^2 <= | x_p - \\mu_j |^2 \\forall_j, 1 \\leq j \\leq k } \n$$\n\n#### Update step: \n\nRecalculate the means (centroids) from the datapoints with in each set S.\n\n$$\n\\mu_i = 1/ S_i \\sum_p x_p\n$$\n\n\n---\n\n## K-means on Town people\nSckit-learn implementation results\n\n![](./img/truth_kmeans.png)\n\n* Predicted Centers: [[-0.27   0.46], [ 5.73,  2.83]]\n* True Centers:  (1, 1), (6, 3)\n\n---\n\n## Gaussian Mixture Model\n\nK-means assumes that the K number of guassians have the same variance (e.g. width)\nGaussian Mixture Model does not assume that.\n\n<br>\n\n### Much Nicer\n\n![](./img/gmm_truth.png){width:900px}\n\n---\n\n## What is Gaussian Mixture Model.\n\n:::{.columns}\n::::{.column width=68%}\n\nBasically, GMM is a generalized k-means clustering algorithm which\nincorporates information about the covariance structure of the data as well as the centers of the \n latent Gaussians (scikit-learn).\n \nA Gaussian mixture model is a probabilistic model that assumes all the \ndata points are generated from a mixture of a finite number of Gaussian \ndistributions with unknown parameters. \n::::\n::::{.column width=30%}\n\n![scikit-learn](./img/sphx_glr_plot_gmm_pdf_001.png)\n[code](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html)\n::::\n:::\n\nEach Gaussian k belongs to a set of clusters where k ∈ {1,…, K}, where K is the number of \nclusters of our dataset. Each k in the mixture comprises the following parameters:\n\n* A mean $\\mu$ that defines its center.\n* A covariance $\\Sigma$ that defines its width. This would be equivalent to the dimensions of an ellipsoid in a multivariate scenario.\n* A mixing probability $\\pi$ that defines how big or small the Gaussian function will be.\n\n\n---\n\n![Toward Data Science](./img/1_lTv7e4Cdlp738X_WFZyZHA.png)\n\nIn addition, the mixing coefficients/probabilities $\\pi$  must sum to 1.  \nE.g. Each data point must exist in one of the k gaussian.\n\n$$\n\\sum_{k=1}^{K} \\pi_k = 1 \n$$\n\n\n---\n\nIn practice, people use a kmeans to determine the initial seeding for the GMM. \nGMM is slower / computationally intensive than kmean.\n\n\n<br>\n<br>\n\n```python\n# Get Data\nA_points, B_points, mu_A, mu_B = define_points()\npoints = np.concatenate((A_points, B_points), axis=0)\n\n# Kmeans\nkmeans = KMeans(init=\"random\", n_clusters=2, n_init=10, max_iter=1000, random_state=42)\nkmeans.fit(points)\n# kmeans_plot(points, kmeans)\n\n# GMM\ngmm = GMM(n_components=2, random_state=0)\ngmm.fit(points)\ngmm_labels = gmm.predict(points)\ngmm_plot(points, kmeans, gmm_labels)\n\n```\n\n<br>\n\n### Who uses GMM \n\nGMM is used for \n\n* recommendations (e.g. which movie you will like)\n* correcting for [air turbulence](https://ecommons.udayton.edu/cgi/viewcontent.cgi?article=1426&context=ece_fac_pub) in imagery\n* We saw in October Modeling Nano Tube production\n\n\n---\n\n## Thank you\n\nThough my code was only two lines, implementing GMM or K-means can be harder\n\n<br>\n\nRemember: The difference between practice and theory is that it is smaller in theory!\n\n<br>\n<br>\n\n### References:\n\n* Scikit-learn: https://scikit-learn.org/stable/modules/mixture.html\n* [GMM](https://scikit-learn.org/stable/modules/mixture.html): https://scikit-learn.org/stable/modules/mixture.html\n* [GMM with nice math](https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95) from Toward Data Science\n* Wikipedia: [K-mean](https://en.wikipedia.org/wiki/K-means_clustering)\n\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["evelyn.css"],"incremental":false,"output-file":"talk.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.1.141","auto-stretch":true,"jupyter":"python3","theme":["default","evelyn.scss"],"logo":"img/DidactexLogo.png","footer":"Gem City Tech ML/AI ","scrollable":true,"transition":"slide","transitionSpeed":"fast","chalkboard":{"buttons":true},"title":"K-mean and GMM Tutorial","subtitle":"Gem City Tech ML/AI Meetup","date":"Nov. 17 2022","author":"Presented by Evelyn J. Boettcher, DiDacTex, LLC","fig-cap-location":"margin"}}}}